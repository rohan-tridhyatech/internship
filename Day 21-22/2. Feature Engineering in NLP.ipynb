{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campusx watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comments</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campusx write comments</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text  output\n",
       "0    people watch campusx       1\n",
       "1   campusx watch campusx       1\n",
       "2   people write comments       0\n",
       "3  campusx write comments       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\" : [\"people watch campusx\", \"campusx watch campusx\",\"people write comments\",\"campusx write comments\"],\n",
    "        \"output\" : [1,1,0,0]\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences 1:\n",
      "people: [0. 0. 1. 0. 0.]\n",
      "watch: [0. 1. 0. 0. 0.]\n",
      "campusx: [1. 0. 0. 0. 0.]\n",
      "Sentences 2:\n",
      "campusx: [1. 0. 0. 0. 0.]\n",
      "watch: [0. 1. 0. 0. 0.]\n",
      "campusx: [1. 0. 0. 0. 0.]\n",
      "Sentences 3:\n",
      "people: [0. 0. 1. 0. 0.]\n",
      "write: [0. 0. 0. 1. 0.]\n",
      "comments: [0. 0. 0. 0. 1.]\n",
      "Sentences 4:\n",
      "campusx: [1. 0. 0. 0. 0.]\n",
      "write: [0. 0. 0. 1. 0.]\n",
      "comments: [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "sentences = df[\"text\"]\n",
    "\n",
    "# Create a vocabulary set\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tfor word in words:\n",
    "\t\tvocab.add(word)\n",
    "\n",
    "# Create a dictionary to map words to integers\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create a binary vector for each word in each sentence\n",
    "vectors = []\n",
    "for sentence in sentences:\n",
    "\twords = sentence.lower().split()\n",
    "\tsentence_vectors = []\n",
    "\tfor word in words:\n",
    "\t\tbinary_vector = np.zeros(len(vocab))\n",
    "\t\tbinary_vector[word_to_int[word]] = 1\n",
    "\t\tsentence_vectors.append(binary_vector)\n",
    "\tvectors.append(sentence_vectors)\n",
    "\n",
    "# Print the one-hot encoded vectors for each word in each sentence\n",
    "for i in range(len(sentences)):\n",
    "\tprint(f\"Sentences {i + 1}:\")\n",
    "\tfor j in range(len(vectors[i])):\n",
    "\t\tprint(f\"{sentences[i].split()[j]}: {vectors[i][j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (Uni-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "bow = cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comments': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0]]\n",
      "[[2 0 0 1 0]]\n",
      "[[0 1 1 0 1]]\n",
      "[[1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())\n",
    "print(bow[3].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"campusx write and watch comments of campusx\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams (Bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "bow = cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch': 2, 'watch campusx': 4, 'campusx watch': 0, 'people write': 3, 'write comments': 5, 'campusx write': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0]]\n",
      "[[1 0 0 0 1 0]]\n",
      "[[0 0 0 1 0 1]]\n",
      "[[0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())\n",
    "print(bow[3].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "bow = cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 4, 'watch': 7, 'campusx': 0, 'people watch': 5, 'watch campusx': 8, 'campusx watch': 1, 'write': 9, 'comments': 3, 'people write': 6, 'write comments': 10, 'campusx write': 2}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 1 0 1 1 0 0]]\n",
      "[[2 1 0 0 0 0 0 1 1 0 0]]\n",
      "[[0 0 0 1 1 0 1 0 0 1 1]]\n",
      "[[1 0 1 1 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())\n",
    "print(bow[3].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49681612, 0.        , 0.61366674, 0.61366674, 0.        ],\n",
       "       [0.8508161 , 0.        , 0.        , 0.52546357, 0.        ],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.57735027],\n",
       "       [0.49681612, 0.61366674, 0.        , 0.        , 0.61366674]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.22314355 1.51082562 1.51082562 1.51082562 1.51082562]\n",
      "['campusx' 'comments' 'people' 'watch' 'write']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)\n",
    "print(tfidf.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
